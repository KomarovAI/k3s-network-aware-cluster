# K3S Network-Aware Cluster ğŸš€

> **Production-ready K3S cluster with intelligent network-aware scheduling, Tailscale integration, and automatic bandwidth/latency optimization for distributed AI workloads**

[![Build Status](https://github.com/KomarovAI/k3s-network-aware-cluster/workflows/CI/badge.svg)](https://github.com/KomarovAI/k3s-network-aware-cluster/actions)
[![Go Report Card](https://goreportcard.com/badge/github.com/KomarovAI/k3s-network-aware-cluster)](https://goreportcard.com/report/github.com/KomarovAI/k3s-network-aware-cluster)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Overview

This project implements an enterprise-grade K3S cluster with **intelligent network-aware scheduling** that automatically:

- ğŸ“Š **Measures network characteristics** between nodes (bandwidth, latency, cost)
- ğŸ§  **Makes smart scheduling decisions** based on real-time network topology
- âš¡ **Optimizes workload placement** to minimize network bottlenecks
- ğŸ”’ **Secures inter-node communication** via Tailscale mesh VPN
- ğŸ“ˆ **Provides comprehensive monitoring** with Prometheus & Grafana

### Key Features

âœ… **Automatic network topology discovery**  
âœ… **Real-time bandwidth and latency monitoring**  
âœ… **Custom K3S scheduler with network awareness**  
âœ… **Tailscale integration for secure mesh networking**  
âœ… **GPU workload optimization for AI/ML tasks**  
âœ… **Production-ready monitoring and alerting**  
âœ… **CI/CD pipeline with automated deployments**  

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VPS Germany   â”‚    â”‚   Home PC #1    â”‚    â”‚   Home PC #2    â”‚
â”‚   (Master)      â”‚    â”‚   (AI Worker)   â”‚    â”‚   (AI Worker)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ K3S Master    â”‚â—„â”€â”€â–ºâ”‚ â€¢ RTX 3090      â”‚â—„â”€â”€â–ºâ”‚ â€¢ RTX 4070      â”‚
â”‚ â€¢ Ingress       â”‚    â”‚ â€¢ Ollama        â”‚    â”‚ â€¢ Stable Diff   â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ 32GB RAM      â”‚    â”‚ â€¢ 16GB RAM      â”‚
â”‚ â€¢ file-pull     â”‚    â”‚ â€¢ NVMe SSD      â”‚    â”‚ â€¢ NVMe SSD      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tailscale Mesh Network â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (10 Mbps â†” 1 Gbps local)
```

## ğŸš€ Quick Start

### Prerequisites

- 2+ Linux machines (VPS + home PCs)
- Docker installed on all nodes
- NVIDIA GPU (optional, for AI workloads)
- Tailscale account

### 1. Setup Tailscale

```bash
# On all nodes
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up --accept-routes --advertise-routes=10.42.0.0/16,10.43.0.0/16
```

### 2. Install K3S Master (VPS)

```bash
git clone https://github.com/KomarovAI/k3s-network-aware-cluster.git
cd k3s-network-aware-cluster

# Run master installation
./scripts/install-master.sh
```

### 3. Install K3S Workers (PCs)

```bash
# Get master token and IP from step 2
export MASTER_IP="100.64.1.5"  # Tailscale IP
export MASTER_TOKEN="K10xxx..."

# Run worker installation
./scripts/install-worker.sh
```

### 4. Deploy Network-Aware Scheduler

```bash
# Apply CRDs and scheduler
kubectl apply -f manifests/network-crds/
kubectl apply -f manifests/scheduler/

kubectl get pods -n kube-system | grep network-aware
```

### 5. Deploy AI Services

```bash
# Deploy AI workloads with network-aware scheduling
kubectl apply -f manifests/applications/ai-services/

# Check pod placement
kubectl get pods -o wide
```

## ğŸ“Š Network-Aware Scheduling

### How It Works

The custom scheduler automatically measures and considers:

| Metric | Description | Impact on Scheduling |
|--------|-------------|---------------------|
| **Bandwidth** | Measured via iperf3 | Pods requiring high data transfer prefer fast nodes |
| **Latency** | Measured via ping RTT | Real-time workloads avoid high-latency paths |
| **Cost** | Calculated from speed | Expensive transfers (slow links) are penalized |
| **Topology** | Node roles and zones | AI workloads prefer GPU nodes, web prefers public nodes |

### Example: Smart Pod Placement

```yaml
# AI workload automatically goes to local GPU node
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  template:
    metadata:
      annotations:
        network.komarov.dev/min-bandwidth: "1mbps"
        network.komarov.dev/max-latency: "100ms"
        network.komarov.dev/data-locality: "high"
    spec:
      schedulerName: network-aware-scheduler  # ğŸ§  Smart scheduling
      containers:
      - name: ollama
        image: ollama/ollama:latest
```

**Result**: Scheduler places Ollama on local PC (1ms latency) instead of VPS (45ms latency)

## ğŸ”§ Configuration

### Network Topology Definition

```yaml
# manifests/network-crds/komarov-topology.yaml
apiVersion: network.komarov.dev/v1
kind: NetworkTopology
metadata:
  name: komarov-network
spec:
  nodes:
    vps-germany:
      bandwidth:
        pc-home: "10mbps"     # Slow internet link
        internet: "1000mbps"  # Fast public access
      latency:
        pc-home: "45ms"       # International latency
        internet: "5ms"       # Local datacenter
      cost:
        pc-home: 0.8          # High cost (slow)
        internet: 0.1         # Low cost (fast)
    pc-home:
      bandwidth:
        vps-germany: "10mbps"
        local: "1000mbps"     # Local network is fast
      latency:
        vps-germany: "45ms"
        local: "1ms"          # Local is instant
      cost:
        vps-germany: 0.8
        local: 0.1
```

## ğŸ“ˆ Monitoring

### Grafana Dashboards

The project includes pre-built dashboards for:

- **Network Topology Map** - Visual representation of node connections
- **Bandwidth Utilization** - Real-time traffic between nodes  
- **Scheduling Decisions** - Why pods were placed where
- **AI Workload Performance** - GPU utilization and task completion times

### Access Monitoring

```bash
# Port-forward Grafana
kubectl port-forward svc/grafana 3000:3000 -n monitoring

# Open http://localhost:3000
# Username: admin
# Password: kubectl get secret grafana -o jsonpath="{.data.admin-password}" | base64 -d
```

## ğŸ§ª Testing

### Network Benchmark

```bash
# Run comprehensive network benchmark
kubectl apply -f tests/network-benchmark.yaml

# View results
kubectl logs job/network-benchmark
```

## ğŸ“¦ Components

### Core Components

| Component | Description | Language | Status |
|-----------|-------------|----------|--------|
| **Network-Aware Scheduler** | Custom K3S scheduler with network intelligence | Go | âœ… Ready |
| **Network Controller** | Monitors and updates network topology | Go | âœ… Ready |
| **Metrics Collector** | Gathers bandwidth/latency data | Go | âœ… Ready |
| **Tailscale Integration** | Secure mesh networking | Shell | âœ… Ready |
| **AI Services** | Ollama, Stable Diffusion, etc. | YAML | âœ… Ready |
| **Monitoring Stack** | Prometheus, Grafana, AlertManager | YAML | âœ… Ready |

### Project Structure

```
â”œâ”€â”€ cluster/                  # Cluster setup scripts
â”‚   â”œâ”€â”€ tailscale/           # Tailscale configuration
â”‚   â”œâ”€â”€ k3s/                 # K3S installation scripts
â”‚   â””â”€â”€ monitoring/          # Monitoring setup
â”œâ”€â”€ scheduler/               # Custom scheduler implementation
â”‚   â”œâ”€â”€ network-aware-scheduler/  # Main scheduler code
â”‚   â”œâ”€â”€ network-controller/      # Network topology controller
â”‚   â””â”€â”€ crds/                    # Custom resource definitions
â”œâ”€â”€ manifests/               # Kubernetes manifests
â”‚   â”œâ”€â”€ scheduler/           # Scheduler deployment
â”‚   â”œâ”€â”€ network-crds/        # Network topology CRDs
â”‚   â”œâ”€â”€ rbac/                # RBAC configuration
â”‚   â””â”€â”€ applications/        # Application deployments
â”œâ”€â”€ docker/                  # Docker images
â”œâ”€â”€ scripts/                 # Installation and utility scripts
â”œâ”€â”€ tests/                   # Test manifests and scripts
â””â”€â”€ docs/                    # Additional documentation
```

## ğŸŒŸ Use Cases

### Perfect For:

- **ğŸ¤– Distributed AI/ML workloads** - Ollama, Stable Diffusion, training jobs
- **ğŸ® Game servers** - Low-latency requirements across regions
- **ğŸ“Š Edge computing** - Processing close to data sources
- **ğŸ  Homelab clusters** - Mixed hardware with varying network speeds
- **â˜ï¸ Hybrid cloud** - On-premises + cloud with intelligent workload placement

### Real-World Example

```yaml
# Automatic optimization:
# â€¢ AI inference runs on local GPU (1ms latency)
# â€¢ Web API runs on VPS (5ms to internet)
# â€¢ Heavy data processing stays local (1Gbps bandwidth)
# â€¢ File sync happens during off-peak (cost optimization)
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Workflow

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Kubernetes SIG-Scheduling** for scheduler framework
- **Tailscale** for excellent mesh networking
- **K3S team** for lightweight Kubernetes
- **Rancher** for K3S and ecosystem tools

## ğŸ“ Support

- ğŸ› **Issues**: [GitHub Issues](https://github.com/KomarovAI/k3s-network-aware-cluster/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/KomarovAI/k3s-network-aware-cluster/discussions)
- ğŸ“§ **Email**: komarov.ai.dev@gmail.com

---

**â­ Star this repo if it helps you build better distributed systems!**