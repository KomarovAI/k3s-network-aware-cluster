# AI Services Deployments with Network-Aware Scheduling

# Ollama - AI Language Model Service
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: default
  labels:
    app: ollama
    workload-type: ai
    component: language-model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
        workload-type: ai
        component: language-model
      annotations:
        network.komarov.dev/min-bandwidth: "1mbps"
        network.komarov.dev/max-latency: "100ms"
        network.komarov.dev/data-locality: "high"
        network.komarov.dev/internet-access: "optional"
    spec:
      schedulerName: network-aware-scheduler
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
          name: http
          protocol: TCP
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0"
        - name: OLLAMA_ORIGINS
          value: "*"
        - name: OLLAMA_NUM_PARALLEL
          value: "4"
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "2"
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "8Gi"
            cpu: "2000m"
          requests:
            memory: "4Gi"
            cpu: "1000m"
        volumeMounts:
        - name: ollama-models
          mountPath: /root/.ollama
        - name: shared-storage
          mountPath: /mnt/shared
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 3
      volumes:
      - name: ollama-models
        hostPath:
          path: /opt/ollama-models
          type: DirectoryOrCreate
      - name: shared-storage
        hostPath:
          path: /mnt/shared
          type: DirectoryOrCreate
      nodeSelector:
        zone: local
        gpu: nvidia
        role: ai-worker
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
# Automatic1111 - Stable Diffusion Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: automatic1111
  namespace: default
  labels:
    app: automatic1111
    workload-type: ai
    component: image-generation
spec:
  replicas: 1
  selector:
    matchLabels:
      app: automatic1111
  template:
    metadata:
      labels:
        app: automatic1111
        workload-type: ai
        component: image-generation
      annotations:
        network.komarov.dev/min-bandwidth: "5mbps"
        network.komarov.dev/max-latency: "50ms"
        network.komarov.dev/data-locality: "high"
        network.komarov.dev/internet-access: "optional"
    spec:
      schedulerName: network-aware-scheduler
      containers:
      - name: webui
        image: komarovai/automatic1111:latest
        ports:
        - containerPort: 7860
          name: webui
          protocol: TCP
        env:
        - name: COMMANDLINE_ARGS
          value: "--listen --api --xformers --enable-insecure-extension-access --skip-torch-cuda-test"
        - name: PYTHONUNBUFFERED
          value: "1"
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4000m"
          requests:
            memory: "8Gi"
            cpu: "2000m"
        volumeMounts:
        - name: sd-models
          mountPath: /app/models
        - name: sd-outputs
          mountPath: /app/outputs
        - name: sd-extensions
          mountPath: /app/extensions
        - name: shared-storage
          mountPath: /mnt/shared
        livenessProbe:
          httpGet:
            path: /api/v1/options
            port: 7860
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/v1/options
            port: 7860
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: sd-models
        hostPath:
          path: /opt/sd-models
          type: DirectoryOrCreate
      - name: sd-outputs
        hostPath:
          path: /opt/sd-outputs
          type: DirectoryOrCreate
      - name: sd-extensions
        hostPath:
          path: /opt/sd-extensions
          type: DirectoryOrCreate
      - name: shared-storage
        hostPath:
          path: /mnt/shared
          type: DirectoryOrCreate
      nodeSelector:
        zone: local
        gpu: nvidia
        role: ai-worker
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
# File-Pull Service - Storage Management
apiVersion: apps/v1
kind: Deployment
metadata:
  name: file-pull
  namespace: default
  labels:
    app: file-pull
    workload-type: storage
    component: file-management
spec:
  replicas: 1
  selector:
    matchLabels:
      app: file-pull
  template:
    metadata:
      labels:
        app: file-pull
        workload-type: storage
        component: file-management
      annotations:
        network.komarov.dev/min-bandwidth: "10mbps"
        network.komarov.dev/max-latency: "50ms"
        network.komarov.dev/internet-access: "required"
    spec:
      schedulerName: network-aware-scheduler
      containers:
      - name: file-pull
        image: komarovai/file-pull:latest
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        env:
        - name: STORAGE_MODE
          value: "unified"
        - name: MAX_FILE_SIZE
          value: "100MB"
        - name: SUPPORTED_FORMATS
          value: "jpg,jpeg,png,gif,bmp,webp,tiff,pdf,txt,md,json"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        volumeMounts:
        - name: unified-storage
          mountPath: /mnt/unified
        - name: cache-storage
          mountPath: /mnt/cache
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: unified-storage
        hostPath:
          path: /mnt/unified
          type: DirectoryOrCreate
      - name: cache-storage
        hostPath:
          path: /mnt/cache
          type: DirectoryOrCreate
      nodeSelector:
        zone: remote
        role: public-gateway

---
# Redis - Task Queue and Caching
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    workload-type: infrastructure
    component: cache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        workload-type: infrastructure
        component: cache
      annotations:
        network.komarov.dev/min-bandwidth: "50mbps"
        network.komarov.dev/max-latency: "20ms"
    spec:
      schedulerName: network-aware-scheduler
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
          name: redis
          protocol: TCP
        command:
        - redis-server
        - --appendonly
        - "yes"
        - --maxmemory
        - "256mb"
        - --maxmemory-policy
        - "allkeys-lru"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: redis-data
          mountPath: /data
        livenessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 15
          periodSeconds: 30
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: redis-data
        hostPath:
          path: /opt/redis-data
          type: DirectoryOrCreate
      nodeSelector:
        zone: remote

---
# AI Task Dispatcher - Load Balancer for AI Services
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-dispatcher
  namespace: default
  labels:
    app: ai-dispatcher
    workload-type: web
    component: api-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-dispatcher
  template:
    metadata:
      labels:
        app: ai-dispatcher
        workload-type: web
        component: api-gateway
      annotations:
        network.komarov.dev/min-bandwidth: "100mbps"
        network.komarov.dev/max-latency: "20ms"
        network.komarov.dev/internet-access: "required"
    spec:
      schedulerName: network-aware-scheduler
      containers:
      - name: dispatcher
        image: komarovai/ai-dispatcher:latest
        ports:
        - containerPort: 5000
          name: http
          protocol: TCP
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: OLLAMA_URL
          value: "http://ollama-service:11434"
        - name: AUTOMATIC1111_URL
          value: "http://automatic1111-service:7860"
        - name: FILE_PULL_URL
          value: "http://file-pull-service:80"
        - name: WORKER_TIMEOUT
          value: "300"
        - name: MAX_CONCURRENT_TASKS
          value: "10"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 15
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 10
      nodeSelector:
        zone: remote
        role: public-gateway

---
# Services for all applications
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  namespace: default
  labels:
    app: ollama
spec:
  selector:
    app: ollama
  ports:
  - name: http
    port: 11434
    targetPort: 11434
    protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: automatic1111-service
  namespace: default
  labels:
    app: automatic1111
spec:
  selector:
    app: automatic1111
  ports:
  - name: webui
    port: 7860
    targetPort: 7860
    protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: file-pull-service
  namespace: default
  labels:
    app: file-pull
spec:
  selector:
    app: file-pull
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: default
  labels:
    app: redis
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: ai-dispatcher-service
  namespace: default
  labels:
    app: ai-dispatcher
spec:
  selector:
    app: ai-dispatcher
  ports:
  - name: http
    port: 5000
    targetPort: 5000
    protocol: TCP
  type: ClusterIP

---
# Ingress for external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ai-services-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Forwarded-Proto $scheme";
      more_set_headers "X-Real-IP $remote_addr";
      more_set_headers "X-Forwarded-For $proxy_add_x_forwarded_for";
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - ai.komarov.dev
    secretName: ai-komarov-tls
  rules:
  - host: ai.komarov.dev
    http:
      paths:
      # AI Dispatcher API
      - path: /api(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: ai-dispatcher-service
            port:
              number: 5000
      # Direct Ollama API access
      - path: /ollama(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: ollama-service
            port:
              number: 11434
      # Direct Stable Diffusion access
      - path: /sd(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: automatic1111-service
            port:
              number: 7860
      # File management
      - path: /files(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: file-pull-service
            port:
              number: 80
      # Default route to dispatcher
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ai-dispatcher-service
            port:
              number: 5000

---
# Network Policies for security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ai-services-netpol
  namespace: default
spec:
  podSelector:
    matchLabels:
      workload-type: ai
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: ai-dispatcher
    - podSelector:
        matchLabels:
          app: nginx-ingress-controller
    ports:
    - protocol: TCP
      port: 11434
    - protocol: TCP
      port: 7860
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to:
    - podSelector:
        matchLabels:
          app: file-pull
    ports:
    - protocol: TCP
      port: 80
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53

---
# Pod Disruption Budget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-services-pdb
  namespace: default
spec:
  minAvailable: 1
  selector:
    matchLabels:
      workload-type: ai