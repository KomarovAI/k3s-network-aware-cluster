#!/usr/bin/env python3
"""
–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ ELK Stack –Ω–∞ worker –Ω–æ–¥–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–µ—Å—É—Ä—Å–æ–≤.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç nodeSelector, —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã –ø–∞–º—è—Ç–∏,
Ingress —Å TLS –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º.

Usage:
  python3 scripts/deploy_elk_on_worker.py --domain cockpit.work.gd --retention-days 15
  python3 scripts/deploy_elk_on_worker.py --domain cockpit.work.gd --retention-days 7 --light-mode
  python3 scripts/deploy_elk_on_worker.py --rollback
"""

import argparse
import json
import subprocess
import sys
import time
import yaml
from pathlib import Path
from typing import Dict, List, Optional

class ELKWorkerDeployer:
    def __init__(self, domain: str, retention_days: int = 15, light_mode: bool = False):
        self.domain = domain
        self.retention_days = retention_days
        self.light_mode = light_mode
        self.namespace = "logging"
        
    def log_info(self, msg: str):
        print(f"‚ÑπÔ∏è  {msg}")
    
    def log_success(self, msg: str):
        print(f"‚úÖ {msg}")
    
    def log_error(self, msg: str):
        print(f"‚ùå {msg}")
    
    def log_warning(self, msg: str):
        print(f"‚ö†Ô∏è  {msg}")
    
    def run_kubectl(self, cmd: str, capture_output=True, check=True) -> subprocess.CompletedProcess:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ kubectl –∫–æ–º–∞–Ω–¥—ã"""
        full_cmd = f"kubectl {cmd}"
        if not capture_output:
            print(f"$ {full_cmd}")
        return subprocess.run(full_cmd, shell=True, capture_output=capture_output, text=True, check=check)
    
    def wait_for_condition(self, cmd: str, success_msg: str, timeout: int = 300) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff"""
        self.log_info(f"–û–∂–∏–¥–∞–Ω–∏–µ: {success_msg}")
        start_time = time.time()
        attempts = 0
        
        while time.time() - start_time < timeout:
            try:
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                if result.returncode == 0:
                    self.log_success(success_msg)
                    return True
            except Exception:
                pass
            
            attempts += 1
            wait_time = min(5 * (1.5 ** (attempts // 3)), 30)
            time.sleep(wait_time)
        
        self.log_error(f"–¢–∞–π–º–∞—É—Ç ({timeout}s) –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏: {success_msg}")
        return False
    
    def check_worker_nodes(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è worker –Ω–æ–¥"""
        try:
            result = self.run_kubectl("get nodes -l node-role.kubernetes.io/worker=worker -o jsonpath='{.items[*].metadata.name}'")
            worker_nodes = result.stdout.strip().split()
            
            if not worker_nodes or worker_nodes == ['']:
                self.log_error("Worker –Ω–æ–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. ELK Stack –Ω—É–∂–Ω–æ —Ä–∞–∑–º–µ—â–∞—Ç—å –Ω–∞ worker!")
                self.log_info("–°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–∫–ª—é—á–∏—Ç–µ worker –Ω–æ–¥—É –∫–æ–º–∞–Ω–¥–æ–π:")
                self.log_info("python3 ~/join_worker_enhanced.py")
                return False
                
            self.log_success(f"–ù–∞–π–¥–µ–Ω—ã worker –Ω–æ–¥—ã: {', '.join(worker_nodes)}")
            return True
            
        except subprocess.CalledProcessError:
            self.log_error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ worker –Ω–æ–¥")
            return False
    
    def create_namespace_and_storage(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ namespace –∏ storage –¥–ª—è ELK"""
        self.log_info("–°–æ–∑–¥–∞–Ω–∏–µ namespace –∏ storage...")
        
        # –°–æ–∑–¥–∞–µ–º namespace
        namespace_yaml = f"""
apiVersion: v1
kind: Namespace
metadata:
  name: {self.namespace}
  labels:
    name: {self.namespace}
    purpose: centralized-logging
"""
        
        # Elasticsearch PVC –¥–ª—è worker
        elasticsearch_storage = "50Gi" if not self.light_mode else "20Gi"
        elasticsearch_pvc = f"""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: elasticsearch-storage
  namespace: {self.namespace}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: {elasticsearch_storage}
"""
        
        # Kibana PVC –¥–ª—è dashboards
        kibana_pvc = f"""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kibana-storage
  namespace: {self.namespace}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
"""
        
        with open("/tmp/elk-namespace-storage.yaml", "w") as f:
            f.write(namespace_yaml + "---" + elasticsearch_pvc + "---" + kibana_pvc)
        
        self.run_kubectl("apply -f /tmp/elk-namespace-storage.yaml", capture_output=False)
        self.log_success("Namespace –∏ storage —Å–æ–∑–¥–∞–Ω—ã")
    
    def deploy_elasticsearch(self):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Elasticsearch –Ω–∞ worker —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏"""
        self.log_info("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Elasticsearch –Ω–∞ worker...")
        
        # –†–µ—Å—É—Ä—Å—ã –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ä–µ–∂–∏–º–∞
        if self.light_mode:
            es_memory_request = "1Gi"
            es_memory_limit = "2Gi" 
            es_cpu_request = "500m"
            es_cpu_limit = "1000m"
            heap_size = "1g"
        else:
            es_memory_request = "2Gi"
            es_memory_limit = "8Gi"  # –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –Ω–∞ –º–æ—â–Ω–æ–º worker
            es_cpu_request = "1000m"
            es_cpu_limit = "4000m"   # –î–æ 4 CPU –Ω–∞ worker
            heap_size = "4g"
        
        elasticsearch_manifest = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: {self.namespace}
  labels:
    app: elasticsearch
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
        component: logging
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "worker"
      containers:
      - name: elasticsearch
        image: elasticsearch:8.11.0
        env:
        - name: discovery.type
          value: single-node
        - name: ES_JAVA_OPTS
          value: "-Xms{heap_size} -Xmx{heap_size}"
        - name: xpack.security.enabled
          value: "false"
        - name: indices.lifecycle.rollover.max_age
          value: "{self.retention_days}d"
        - name: cluster.routing.allocation.disk.threshold_enabled
          value: "true"
        - name: cluster.routing.allocation.disk.watermark.low
          value: "85%"
        - name: cluster.routing.allocation.disk.watermark.high
          value: "90%"
        resources:
          requests:
            cpu: {es_cpu_request}
            memory: {es_memory_request}
          limits:
            cpu: {es_cpu_limit}
            memory: {es_memory_limit}
        ports:
        - containerPort: 9200
        - containerPort: 9300
        volumeMounts:
        - name: elasticsearch-storage
          mountPath: /usr/share/elasticsearch/data
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 30
          timeoutSeconds: 30
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          timeoutSeconds: 30
      volumes:
      - name: elasticsearch-storage
        persistentVolumeClaim:
          claimName: elasticsearch-storage
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: {self.namespace}
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
    name: http
  - port: 9300
    targetPort: 9300
    name: transport
"""
        
        with open("/tmp/elasticsearch.yaml", "w") as f:
            f.write(elasticsearch_manifest)
        
        self.run_kubectl("apply -f /tmp/elasticsearch.yaml", capture_output=False)
        
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status deployment/elasticsearch --timeout=300s",
            "Elasticsearch –≥–æ—Ç–æ–≤ –Ω–∞ worker",
            timeout=360
        ):
            self.log_success(f"Elasticsearch —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç –Ω–∞ worker —Å {es_memory_limit} RAM")
    
    def deploy_logstash(self):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Logstash –Ω–∞ worker"""
        self.log_info("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Logstash –Ω–∞ worker...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Logstash
        logstash_config = f"""
input {{
  beats {{
    port => 5044
  }}
  http {{
    port => 8080
  }}
}}

filter {{
  if [kubernetes] {{
    mutate {{
      add_field => {{ "cluster" => "k3s-hybrid" }}
    }}
  }}
  
  # –ü–∞—Ä—Å–∏–Ω–≥ JSON –ª–æ–≥–æ–≤
  if [message] =~ /^\\{{/ {{
    json {{
      source => "message"
      target => "json_data"
    }}
  }}
  
  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
  date {{
    match => [ "timestamp", "ISO8601" ]
  }}
}}

output {{
  elasticsearch {{
    hosts => ["elasticsearch:9200"]
    index => "logs-k3s-%{{+YYYY.MM.dd}}"
    template_overwrite => true
    template_name => "k3s-logs"
  }}
  
  # Debug output
  stdout {{ codec => rubydebug }}
}}
"""
        
        # ConfigMap –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        logstash_configmap = {
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": "logstash-config",
                "namespace": self.namespace
            },
            "data": {
                "logstash.conf": logstash_config.strip()
            }
        }
        
        # Logstash Deployment
        logstash_memory = "512Mi" if self.light_mode else "1Gi"
        logstash_cpu = "200m" if self.light_mode else "500m"
        
        logstash_manifest = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: {self.namespace}
  labels:
    app: logstash
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
        component: logging
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "worker"
      containers:
      - name: logstash
        image: logstash:8.11.0
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx{logstash_memory.replace('i', '').replace('G', 'g').replace('M', 'm')}"
        resources:
          requests:
            cpu: {logstash_cpu}
            memory: {logstash_memory}
          limits:
            cpu: 1000m
            memory: {logstash_memory.replace('512Mi', '1Gi')}
        ports:
        - containerPort: 5044
        - containerPort: 8080
        volumeMounts:
        - name: logstash-config
          mountPath: /usr/share/logstash/pipeline
      volumes:
      - name: logstash-config
        configMap:
          name: logstash-config
---
apiVersion: v1
kind: Service
metadata:
  name: logstash
  namespace: {self.namespace}
spec:
  selector:
    app: logstash
  ports:
  - port: 5044
    targetPort: 5044
    name: beats
  - port: 8080
    targetPort: 8080
    name: http
"""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–µ–ø–ª–æ–π
        with open("/tmp/logstash-configmap.yaml", "w") as f:
            yaml.dump(logstash_configmap, f)
        
        with open("/tmp/logstash.yaml", "w") as f:
            f.write(logstash_manifest)
        
        self.run_kubectl("apply -f /tmp/logstash-configmap.yaml", capture_output=False)
        self.run_kubectl("apply -f /tmp/logstash.yaml", capture_output=False)
        
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status deployment/logstash --timeout=180s",
            "Logstash –≥–æ—Ç–æ–≤ –Ω–∞ worker"
        ):
            self.log_success(f"Logstash —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç –Ω–∞ worker —Å {logstash_memory} RAM")
    
    def deploy_kibana(self):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Kibana –Ω–∞ worker —Å Ingress"""
        self.log_info("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Kibana –Ω–∞ worker...")
        
        kibana_memory = "256Mi" if self.light_mode else "512Mi"
        
        kibana_manifest = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: {self.namespace}
  labels:
    app: kibana
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
        component: logging
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "worker"
      containers:
      - name: kibana
        image: kibana:8.11.0
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: SERVER_HOST
          value: "0.0.0.0"
        - name: SERVER_PUBLICBASEURL
          value: "https://kibana.{self.domain}"
        - name: LOGGING_ROOT_LEVEL
          value: "warn"
        resources:
          requests:
            cpu: 200m
            memory: {kibana_memory}
          limits:
            cpu: 1000m
            memory: {kibana_memory.replace('256Mi', '512Mi')}
        ports:
        - containerPort: 5601
        volumeMounts:
        - name: kibana-storage
          mountPath: /usr/share/kibana/data
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
          timeoutSeconds: 30
      volumes:
      - name: kibana-storage
        persistentVolumeClaim:
          claimName: kibana-storage
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: {self.namespace}
spec:
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana
  namespace: {self.namespace}
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - kibana.{self.domain}
    secretName: kibana-tls
  rules:
  - host: kibana.{self.domain}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana
            port:
              number: 5601
"""
        
        with open("/tmp/kibana.yaml", "w") as f:
            f.write(kibana_manifest)
        
        self.run_kubectl("apply -f /tmp/kibana.yaml", capture_output=False)
        
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status deployment/kibana --timeout=240s",
            "Kibana –≥–æ—Ç–æ–≤–∞ –Ω–∞ worker"
        ):
            self.log_success(f"Kibana –¥–æ—Å—Ç—É–ø–Ω–∞: https://kibana.{self.domain}")
    
    def deploy_filebeat_daemonset(self):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Filebeat DaemonSet –¥–ª—è —Å–±–æ—Ä–∞ –ª–æ–≥–æ–≤ —Å –≤—Å–µ—Ö –Ω–æ–¥"""
        self.log_info("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Filebeat DaemonSet...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Filebeat
        filebeat_config = f"""
filebeat.autodiscover:
  providers:
    - type: kubernetes
      node: ${{NODE_NAME}}
      hints.enabled: true
      hints.default_config:
        type: container
        paths:
          - /var/log/containers/*${{data.kubernetes.container.id}}.log

processors:
  - add_kubernetes_metadata:
      host: ${{NODE_NAME}}
      matchers:
      - logs_path:
          logs_path: "/var/log/containers/"

# –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Logstash –Ω–∞ worker
output.logstash:
  hosts: ["logstash.{self.namespace}.svc.cluster.local:5044"]
  
# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤ Elasticsearch –Ω–∞–ø—Ä—è–º—É—é (fallback)
output.elasticsearch:
  hosts: ["elasticsearch.{self.namespace}.svc.cluster.local:9200"]
  index: "filebeat-k3s-%{{+yyyy.MM.dd}}"

logging.level: warning
"""
        
        filebeat_configmap = {
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": "filebeat-config",
                "namespace": self.namespace
            },
            "data": {
                "filebeat.yml": filebeat_config.strip()
            }
        }
        
        # Filebeat DaemonSet (–Ω–∞ –≤—Å–µ—Ö –Ω–æ–¥–∞—Ö)
        filebeat_manifest = f"""
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: {self.namespace}
  labels:
    app: filebeat
    component: logging
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
        component: logging
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: filebeat
        image: elastic/filebeat:8.11.0
        args:
        - "-c"
        - "/etc/filebeat.yml"
        - "-e"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        securityContext:
          runAsUser: 0
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0640
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
      tolerations:
      - operator: Exists  # –ú–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –≤—Å–µ—Ö –Ω–æ–¥–∞—Ö –≤–∫–ª—é—á–∞—è master
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: {self.namespace}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - pods
  - nodes
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: {self.namespace}
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
"""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open("/tmp/filebeat-configmap.yaml", "w") as f:
            yaml.dump(filebeat_configmap, f)
        
        with open("/tmp/filebeat.yaml", "w") as f:
            f.write(filebeat_manifest)
        
        self.run_kubectl("apply -f /tmp/filebeat-configmap.yaml", capture_output=False)
        self.run_kubectl("apply -f /tmp/filebeat.yaml", capture_output=False)
        
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status daemonset/filebeat --timeout=120s",
            "Filebeat DaemonSet –≥–æ—Ç–æ–≤"
        ):
            self.log_success("Filebeat —Å–æ–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏ —Å–æ –≤—Å–µ—Ö –Ω–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞")
    
    def deploy_optional_components(self):
        """–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (Jaeger, External monitoring)"""
        self.log_info("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # Jaeger All-in-One –Ω–∞ worker
        jaeger_manifest = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: {self.namespace}
  labels:
    app: jaeger
    component: tracing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
        component: tracing
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "worker"
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.50
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: SPAN_STORAGE_TYPE
          value: "memory"  # –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ Elasticsearch
        resources:
          requests:
            cpu: 200m
            memory: 384Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        ports:
        - containerPort: 16686  # UI
        - containerPort: 14268  # HTTP collector
        - containerPort: 6831   # UDP collector
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: {self.namespace}
spec:
  selector:
    app: jaeger
  ports:
  - port: 16686
    targetPort: 16686
    name: ui
  - port: 14268
    targetPort: 14268
    name: http
  - port: 6831
    targetPort: 6831
    name: udp
    protocol: UDP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jaeger
  namespace: {self.namespace}
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - jaeger.{self.domain}
    secretName: jaeger-tls
  rules:
  - host: jaeger.{self.domain}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: jaeger
            port:
              number: 16686
"""
        
        # Blackbox Exporter –¥–ª—è external monitoring
        blackbox_manifest = f"""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blackbox-exporter
  namespace: {self.namespace}
  labels:
    app: blackbox-exporter
    component: external-monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blackbox-exporter
  template:
    metadata:
      labels:
        app: blackbox-exporter
        component: external-monitoring
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9115"
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: "worker"
      containers:
      - name: blackbox-exporter
        image: prom/blackbox-exporter:v0.24.0
        resources:
          requests:
            cpu: 50m
            memory: 96Mi
          limits:
            cpu: 200m
            memory: 256Mi
        ports:
        - containerPort: 9115
---
apiVersion: v1
kind: Service
metadata:
  name: blackbox-exporter
  namespace: {self.namespace}
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9115"
spec:
  selector:
    app: blackbox-exporter
  ports:
  - port: 9115
    targetPort: 9115
"""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        with open("/tmp/jaeger.yaml", "w") as f:
            f.write(jaeger_manifest)
        
        with open("/tmp/blackbox-exporter.yaml", "w") as f:
            f.write(blackbox_manifest)
        
        self.run_kubectl("apply -f /tmp/jaeger.yaml", capture_output=False)
        self.run_kubectl("apply -f /tmp/blackbox-exporter.yaml", capture_output=False)
        
        # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status deployment/jaeger --timeout=120s",
            "Jaeger –≥–æ—Ç–æ–≤"
        ):
            self.log_success(f"Jaeger UI: https://jaeger.{self.domain}")
        
        if self.wait_for_condition(
            f"kubectl -n {self.namespace} rollout status deployment/blackbox-exporter --timeout=60s",
            "Blackbox Exporter –≥–æ—Ç–æ–≤"
        ):
            self.log_success("External monitoring –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
    def create_index_templates(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ index templates –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Elasticsearch"""
        self.log_info("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Elasticsearch index templates...")
        
        # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Elasticsearch
        if not self.wait_for_condition(
            f"kubectl -n {self.namespace} exec deployment/elasticsearch -- curl -s http://localhost:9200/_cluster/health",
            "Elasticsearch API –≥–æ—Ç–æ–≤"
        ):
            return
        
        # Index template –¥–ª—è –ª–æ–≥–æ–≤ K3S
        index_template = {
            "index_patterns": ["logs-k3s-*", "filebeat-k3s-*"],
            "template": {
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,  # Single node
                    "index.lifecycle.name": "k3s-logs-policy",
                    "index.lifecycle.rollover_alias": "k3s-logs"
                },
                "mappings": {
                    "properties": {
                        "@timestamp": {"type": "date"},
                        "kubernetes.namespace": {"type": "keyword"},
                        "kubernetes.pod.name": {"type": "keyword"},
                        "kubernetes.container.name": {"type": "keyword"},
                        "message": {"type": "text", "analyzer": "standard"}
                    }
                }
            }
        }
        
        with open("/tmp/index-template.json", "w") as f:
            json.dump(index_template, f, indent=2)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —á–µ—Ä–µ–∑ kubectl exec
        template_cmd = f"""kubectl -n {self.namespace} exec deployment/elasticsearch -- curl -X PUT 'localhost:9200/_index_template/k3s-logs-template' -H 'Content-Type: application/json' -d '@-' < /tmp/index-template.json"""
        result = subprocess.run(template_cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            self.log_success("Index templates –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        
        # Index Lifecycle Policy –¥–ª—è –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∏
        lifecycle_policy = {
            "policy": {
                "phases": {
                    "hot": {
                        "actions": {
                            "rollover": {
                                "max_size": "5GB",
                                "max_age": "1d"
                            }
                        }
                    },
                    "delete": {
                        "min_age": f"{self.retention_days}d"
                    }
                }
            }
        }
        
        with open("/tmp/lifecycle-policy.json", "w") as f:
            json.dump(lifecycle_policy, f, indent=2)
        
        lifecycle_cmd = f"""kubectl -n {self.namespace} exec deployment/elasticsearch -- curl -X PUT 'localhost:9200/_ilm/policy/k3s-logs-policy' -H 'Content-Type: application/json' -d '@-' < /tmp/lifecycle-policy.json"""
        subprocess.run(lifecycle_cmd, shell=True, capture_output=True)
        
        self.log_success(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ {self.retention_days} –¥–Ω–µ–π")
    
    def setup_elk_monitoring_integration(self):
        """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è ELK —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º Prometheus –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º"""
        self.log_info("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Prometheus...")
        
        # ServiceMonitor –¥–ª—è Elasticsearch
        service_monitor = f"""
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: elasticsearch
  namespace: {self.namespace}
  labels:
    app: elasticsearch
spec:
  selector:
    matchLabels:
      app: elasticsearch
  endpoints:
  - port: http
    interval: 30s
    path: /_prometheus/metrics
"""
        
        with open("/tmp/elasticsearch-servicemonitor.yaml", "w") as f:
            f.write(service_monitor)
        
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å ServiceMonitor (–µ—Å–ª–∏ –µ—Å—Ç—å Prometheus Operator)
        result = self.run_kubectl("apply -f /tmp/elasticsearch-servicemonitor.yaml", check=False, capture_output=True)
        
        if result.returncode == 0:
            self.log_success("ServiceMonitor –¥–ª—è Elasticsearch —Å–æ–∑–¥–∞–Ω")
        else:
            self.log_info("ServiceMonitor –Ω–µ —Å–æ–∑–¥–∞–Ω (Prometheus Operator –Ω–µ –Ω–∞–π–¥–µ–Ω)")
    
    def rollback_elk_stack(self):
        """–û—Ç–∫–∞—Ç ELK Stack"""
        self.log_info("–û—Ç–∫–∞—Ç ELK Stack...")
        
        try:
            self.run_kubectl(f"delete namespace {self.namespace}", capture_output=False)
            self.log_success("ELK Stack —É–¥–∞–ª–µ–Ω")
            return True
        except subprocess.CalledProcessError:
            self.log_error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ ELK Stack")
            return False
    
    def run_smoke_tests(self):
        """Smoke tests –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ ELK Stack"""
        self.log_info("–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ smoke tests...")
        
        tests = [
            (f"kubectl -n {self.namespace} get pods", "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–¥–æ–≤ ELK"),
            (f"kubectl -n {self.namespace} exec deployment/elasticsearch -- curl -s localhost:9200/_cluster/health", "–ü—Ä–æ–≤–µ—Ä–∫–∞ Elasticsearch health"),
            (f"kubectl -n {self.namespace} get ingress", "–ü—Ä–æ–≤–µ—Ä–∫–∞ Ingress –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"),
            ("kubectl get certificates --all-namespaces | grep kibana", "–ü—Ä–æ–≤–µ—Ä–∫–∞ TLS —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤")
        ]
        
        success_count = 0
        for cmd, description in tests:
            if self.wait_for_condition(cmd, description, timeout=60):
                success_count += 1
        
        self.log_info(f"Smoke tests: {success_count}/{len(tests)} –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ")
        
        if success_count == len(tests):
            print(f"\nüéâ ELK STACK –†–ê–ó–í–ï–†–ù–£–¢ –£–°–ü–ï–®–ù–û!")
            print(f"üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã:")
            print(f"  ‚Ä¢ Kibana (–ª–æ–≥–∏ –∏ –¥–∞—à–±–æ—Ä–¥—ã): https://kibana.{self.domain}")
            if not self.light_mode:
                print(f"  ‚Ä¢ Jaeger (—Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞): https://jaeger.{self.domain}")
            print(f"\nüí° –ü–µ—Ä–≤—ã–µ —à–∞–≥–∏:")
            print(f"  1. –û—Ç–∫—Ä–æ–π—Ç–µ Kibana –∏ —Å–æ–∑–¥–∞–π—Ç–µ index pattern: logs-k3s-*")
            print(f"  2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –≤ Index Management")
            print(f"  3. –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π")
    
    def deploy_full_elk_stack(self) -> bool:
        """–ü–æ–ª–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ ELK Stack –Ω–∞ worker"""
        print("üöÄ –†–ê–ó–í–ï–†–¢–´–í–ê–ù–ò–ï ELK STACK –ù–ê WORKER –ù–û–î–ï")
        print("="*60)
        print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –¥–æ–º–µ–Ω={self.domain}, retention={self.retention_days}d, light_mode={self.light_mode}")
        print()
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ worker –Ω–æ–¥
            if not self.check_worker_nodes():
                return False
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ namespace –∏ storage
            self.create_namespace_and_storage()
            
            # 3. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Elasticsearch
            self.deploy_elasticsearch()
            
            # 4. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Logstash
            self.deploy_logstash()
            
            # 5. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Kibana —Å Ingress
            self.deploy_kibana()
            
            # 6. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Filebeat DaemonSet
            self.deploy_filebeat_daemonset()
            
            # 7. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            if not self.light_mode:
                self.deploy_optional_components()
            
            # 8. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ index templates
            time.sleep(30)  # –î–∞–µ–º –≤—Ä–µ–º—è Elasticsearch –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è
            self.create_index_templates()
            
            # 9. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            self.setup_elk_monitoring_integration()
            
            # 10. Smoke tests
            self.run_smoke_tests()
            
            return True
            
        except KeyboardInterrupt:
            self.log_warning("–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return False
        except Exception as e:
            self.log_error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ ELK Stack –Ω–∞ worker –Ω–æ–¥–µ")
    parser.add_argument("--domain", required=True, help="–ë–∞–∑–æ–≤—ã–π –¥–æ–º–µ–Ω –¥–ª—è Ingress")
    parser.add_argument("--retention-days", type=int, default=15, help="–°—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤ (–¥–Ω–µ–π)")
    parser.add_argument("--light-mode", action="store_true", help="–û–±–ª–µ–≥—á–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º (–º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)")
    parser.add_argument("--rollback", action="store_true", help="–£–¥–∞–ª–∏—Ç—å ELK Stack")
    
    args = parser.parse_args()
    
    if args.rollback:
        deployer = ELKWorkerDeployer("", 0)
        success = deployer.rollback_elk_stack()
    else:
        deployer = ELKWorkerDeployer(args.domain, args.retention_days, args.light_mode)
        success = deployer.deploy_full_elk_stack()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()